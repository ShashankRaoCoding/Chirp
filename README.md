
# 🐦 Chirp: Whisper with AI

**Chirp** is a sleek, transparent desktop overlay that provides a whisper-quiet interface for chatting with a local LLaMA 3 model via Ollama.

No browser tabs. No busy windows. Just a discreet, always-on-top assistant that listens when summoned and fades away when you're done.

---

## ✨ Features

+ 🖥️ Transparent UI with no visible frame
+ 🧠 Connects to local LLaMA 3 model via Ollama
+ 📡 Streams responses in real time
+ 📌 Always-on-top, autohide when not in use
+ 🖱️ Reappears when the mouse approaches the bottom of the screen
+ ⚙️ Minimal configuration, easy to run
+ 📦 Powered by [Astilectron](https://github.com/asticode/go-astilectron) + Go + HTML/CSS + Javascript 

---

## 🚀 Getting Started

### Prerequisites

+ [Go](https://go.dev/dl/)
+ [Ollama](https://ollama.com) installed and configured
+ LLaMA 3 model pulled via:

```bash
  ollama pull llama3
````

---

### 🛠 Build and Run

Double click on the main.exe file in the folder. Alternatively: 

```bash
go run main.go
```

Or to build:

```bash
go build -o chirp.exe
./chirp.exe
```

Chirp will:

1. Automatically start the `ollama serve` process in the background.
2. Launch a transparent always-on-top window.
3. Watch for your cursor to approach the screen bottom and slide into view.

---

## 📂 Project Structure

| File / Folder     | Description                                                    |
| ----------------- | -------------------------------------------------------------- |
| `main.go`         | Main Go application with embedded HTTP server and chat routing |
| `pages/chat.html` | Your frontend chat interface                                   |
| `shankskit`       | Custom UI/bootstrapping package for Astilectron                |
| `go.mod`          | Module definition                                              |

---

## 🧠 How It Works

1. You send a prompt from the frontend.
2. Go backend sends it to Ollama’s local `/generate` API with streaming enabled.
3. Response tokens are streamed back to the UI live.

---

## 🙌 Credits

* Built using [Astilectron](https://github.com/asticode/go-astilectron)
* Local LLM served by [Ollama](https://ollama.com)
* Window auto-hide by monitoring Windows cursor position with syscall

---

## 🧪 Future Features (Ideas)

* 🔊 Voice-to-text input
* 💬 Multi-chat memory
* 🎨 Themed UI options
* 🖥️ Screen-edge pinning for other sides
* 🔐 Local encryption of chat history

---

## 🐛 Known Issues

* Only tested on **Windows**
* Requires LLaMA model and Ollama to be working locally

---

## 📬 Contact

Made with ❤️ by [@shashankraocoding](https://github.com/shashankraocoding)
For questions or ideas, feel free to open an issue or PR!

---


---

### ⚠️ Liabilities

Chirp is provided **as-is**, with no guarantees or warranties. By using this application, you acknowledge and accept the following:

* 🧠 **AI Output Responsibility**: The responses generated by the LLaMA model via Ollama are not moderated. You are solely responsible for interpreting and using the output.
* 🛡️ **Data Privacy**: This app runs locally and does not transmit data externally. However, **you** are responsible for safeguarding any sensitive prompts or responses.
* ⚠️ **Experimental Nature**: Chirp is an experimental interface for local AI interaction. It is not intended for critical, medical, legal, or safety-related use.
* 🖥️ **Platform Limitations**: Chirp has only been tested on **Windows**. Behavior on other platforms is unverified and unsupported.
* 🔧 **System Resources**: Running large language models locally can be resource-intensive. Monitor your system performance and usage while using this tool.

By using Chirp, you agree that the developers and contributors are **not liable** for any damages, data loss, or issues arising from its use, including as a result of negligence.

---

